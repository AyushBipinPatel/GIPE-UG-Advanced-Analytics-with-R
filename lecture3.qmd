---
title: "Classificaiton"
subtitle: "Advance Analytics with R (UG 21-24)"
author: "Ayush Patel"
format: 
  revealjs:
    embed-resources: true
    slide-number: c/t
    width: 1400
    theme: [serif, theme.scss]
---
```{r echo=FALSE,message=FALSE,warning=FALSE}

library(tidyverse)
library(MASS)
library(ISLR)
library(ISLR2)
library(patchwork)

```


## Before we start

::: columns
::: {.column width="70%"}

Please load the following packages

```{r}
#| eval: false
#| echo: true

library(tidyverse)
library(MASS)
library(ISLR)
library(ISLR2)
library(nnet)### get this if you don't
library(e1071) ## get this if you don't

```

<br> <br>

Access lecture slide from [bit.ly/aar-ug](http://bit.ly/aar-ug)
:::

::: {.column width="30%"}
```{=html}
<figure>
<img src="https://images.metmuseum.org/CRDImages/aa/original/DT5334.jpg" alt="Warrior's armor(gusoku)" >
<figcaption style="font-size:10px;">Source: <a herf = "https://www.metmuseum.org/art/collection/search/24975?searchField=All&amp;sortBy=Relevance&amp;high=on&amp;ao=on&amp;showOnly=openAccess&amp;ft=start&amp;offset=0&amp;rpp=40&amp;pos=7">Armor (Gusoku)</a></figcaption>
</figure>
```
:::
:::

## Hello

::: columns
::: {.column width="70%"}
I am [Ayush]{.fragment fragment-index="1" style="font-size:45px"}.

[I am a researcher working at the intersection of data, law, development and economics.]{.fragment fragment-index="2" style="font-size:25px"}

[I teach Data Science using R at Gokhale Institute of Politics and Economics]{.fragment fragment-index="3" style="font-size:25px"}

[I am a [RStudio (Posit) certified tidyverse Instructor.](https://education.rstudio.com/trainers/people/patel+ayush/)]{.fragment fragment-index="4" style="font-size:25px"}

[I am a Researcher at [Oxford Poverty and Human development Initiative (OPHI)](https://ophi.org.uk/), at the University of Oxford.]{.fragment fragment-index="5" style="font-size:25px"}
:::

::: {.column width="30%"}
**Reach me**

{{< fa solid envelope >}} [ayush.ap58\@gmail.com]{style="font-size:25px"}

{{< fa solid envelope >}} [ayush.patel\@gipe.ac.in]{style="font-size:25px"}
:::
:::

## Learning Objective {.center}

*Dip* our toes into classification techniques. How to apply and assess these methods.

References for this lecture:

-   Chapter 4, ISLR (reference)
-   Chapters 9, Intro to Modern Statistics (Reading for intuitive understanding)
-   Chapter 10.2 Modern Data Science with R

## What is Classification?



  - Predict qualitative response
  - Approaches of predicting qualitative response, a process called *classification.*
  - A method or technique can be referred to as a *classifier*.
  - We will look into: *logistic regression, linear discriminant analysis, quadratic discriminant analysis, naive Bayes and K-nearest neighbours*


## What actually happens{.smaller}

>"*....often the methods used for classification first predict the probability that the observation belongs to each of the categories of a qualitative  variable, as the basis for making the classification. In this sense they also behave like regression methods.*"

### Why not use linear regression??

   - Nominal categorical variables have no rank. How to provide quantitative values?
   - Distance between Ordinal variable values are not easy to assign.
   - Could do something when the response is nominal with only two levels.
   - No guarantee that our estimates will be between [0,1]. Makes interpreting probabilities difficult.
   
## `Default` data{.scrollable}

```{r}
#| echo: false

head(Default,20)|>
  knitr::kable()|>
  kableExtra::kable_styling()|>
  kableExtra::scroll_box(width = "100%")
  

```

## Logistic Regression

  - Logistic regressions are well suited for qualitative binary responses.
  - *default* variable from `Default` is our response($Y$).
  - It has two levels `Yes` or `No`.
  - We model the probability that $Y$ belongs to one a particular category.
  - $Pr(default = Yes|balance)$ - logistic model estimates this. Is referred to as $p(balance)$ as well.
  - Mainly, depending on risk aversion behaviour, $a$ is chosen. $p(balance) > a$, where $0<=a<=1$.
  
## But what if ? {.scrollable}

**I ran this:** $p(balance) = \beta_0 + \beta_1X$

:::{.panel-tabset}

### Code

```{r}
#| echo: true

## make a dummy for default

Default|>
  mutate(
    default_dumm = ifelse(
      default == "Yes",
      1,0
    )
  )-> def_dum

## regress dummy over balance and plot 

lm(default_dumm ~ balance, 
   data = def_dum)|>
  broom::augment()|>
  ggplot(aes(balance,default_dumm))+
  geom_point(alpha= 0.6)+
  geom_line(aes(balance, .fitted),
            colour = "red")+
  labs(
    title = "Linear regression fit to qualitative response",
    subtitle = "Yes =1, No = 0",
    y = "prob default status"
  )+
  theme_minimal() -> plot_linear

## Run the logistic regression

glm(
  default_dumm ~ balance,
  data = def_dum,
  family = binomial
)|>
  broom::augment(type.predict = "response")|>
  ggplot(aes(balance,default_dumm))+
  geom_point(alpha= 0.6)+
  geom_line(aes(balance, .fitted),
            colour = "red")+
  labs(
    title = "Logistic regression fit to qualitative response",
    subtitle = "Yes =1, No = 0",
    y = "prob default status"
  )+
  theme_minimal() -> logistic_plot

```


### Output

```{r}
#| echo: false
#| fig-align: center

plot_linear + logistic_plot
```


:::

## Logistic Model{.scrollable}

We saw that some fitted values in the **linear model** were **negative**.

We need a function that will return values between `[0,1]`.

$$p(X) = \frac{e^{(\beta_0 + \beta_1X)}}{1+e^{\beta_0 + \beta_1X}}$$

This is the *logistic function*, modeled by the *maximum likelihood* method.

**odds**:

$$\frac{p(X)}{1-p(X)}$$
**log odds or logit:

$$log(\frac{p(X)}{1-p(X)}) = \beta_0 + \beta_1X$$
 
 
## Exercise - concept

if the following are the results of the model $logit(p(default)) = \beta_0 + \beta_1Balance$:

```{r}
#| echo: false

glm(
  default_dumm ~ balance,
  data = def_dum,
  family = binomial,
)|>
  broom::tidy()|>
  gt::gt()
```
**What is the probability of default with balance $5000??**

## Multiple logistic Regression

$$p(X) = \frac{e^{(\beta_0 + \beta_1X_1 + \beta_2X_2+...+\beta_nX_n)}}{1+e^{\beta_0 + \beta_1X_1 + \beta_2X_2+...+\beta_nX_n}}$$

```{r}


glm(
  default_dumm ~ income + balance + student,
  data = def_dum,
  family = binomial
)|>
  broom::tidy()|>
  gt::gt()|>
  gt::tab_options(
    table.font.size = 20,
    table.width = gt::pct(50)
  )

glm(
  default_dumm ~ student,
  data = def_dum,
  family = binomial
)|>
  broom::tidy()|>
  gt::gt()|>
  gt::tab_options(
    table.font.size = 20,
    table.width = gt::pct(50)
  )

```

## How to know if its good?

> There is no consesus in statistics community over a single measure that can describe a goodness of fit for logistic regression.

```{r}
#| echo: true

glm(
  default_dumm ~ income + balance + student,
  data = def_dum,
  family = binomial
) -> mod_logit

DescTools::PseudoR2(mod_logit,
                    which = c("McFadden", "CoxSnell",
                              "Nagelkerke", "Tjur"))


```

```{r}
#| echo: true

AIC(mod_logit) # be careful with this

```

## Exercise

Use the `Credit` data in {ISLR}. 

  - Create three new variables :
    - one: mark_south (1 if Region is South, else 0)
    - Two: mark_west (1 if Region is West, else 0)
    - Three: mark_east (1 if Region is East, else 0)
    
  - Create three binomial logistic models, one for each newly created variable.
  - Get $PseudoR^2$ for each model.

##  {.center}

>**What you just did is called *Stratified binary model*.**

  - n models are created to understand probabilities related to n levels of the categorical response variable.
  - n models are non-comparable.
  - relative probabilities amongst n levels of the response are not known.
  
## Relative risk or Baseline Approach
*to Multinomial Logistic Regression*

$$Pr(Y=k|X=x) = \frac{e^{\beta_{k0}+\beta_{k1}x_1+...+\beta_{kp}xp}}{1+\sum_{l=1}^{K-1}e^{\beta_{l0}+\beta_{l1}x_1+...+\beta_{lp}x_p}}$$

for k = 1,...K-1, and

$$Pr(Y=K|X=x) = \frac{1}{1+\sum_{l=1}^{K-1}e^{\beta_{l0}+\beta_{l1}x_1+...+\beta_{lp}x_p}}$$

## Multinomial Logistic

$$log(\frac{Pr(Y=k|X=x)}{Pr(Y=K|X=x)}) = \beta_{k0}+\beta_{k1}x_1+...+\beta_{kp}xp$$

 - Which class is treated as reference or baseline is unimportant.
 
 - How to interpret this?
 
## Data 

```{r}
palmerpenguins::penguins|>
  dplyr::glimpse()
```

## Set Baseline/reference

```{r}
#| echo: true

palmerpenguins::penguins|>
  mutate(
    species = stats::relevel(species,
                             ref = "Gentoo")
  ) -> peng_ref

levels(peng_ref$species)

```

## Describe model

```{r}
#| echo: true



multi_log <- nnet::multinom(
  formula = species ~ body_mass_g + bill_length_mm + bill_depth_mm + flipper_length_mm + sex + island, 
  data = peng_ref
)
```

## Peek into Summary - notice anything?

```{r}
summary(multi_log)
```

## Getting p-values

```{r}
#| echo: true

# calculate z-statistics of coefficients
z_stats <- summary(multi_log)$coefficients/
  summary(multi_log)$standard.errors

# convert to p-values
p_values <- (1 - pnorm(abs(z_stats)))*2


# display p-values in transposed data frame
data.frame(t(p_values))
```

## Fitted values{.scorllable}

```{r}
multi_log$fitted.values
```

## Exercise

Use the Publication data from ISLR2.

Split data into 80%-20% training and test set randomly. 

Generate a multinomial logistic model to classify variable mech. 

use the test data to predict mech variable. See if it is a reasonable fit.

## Test Error Rate

What was the test error rate you get for the previous exercise?

$$Ave(I(y_0 \neq \hat y_0))$$

## The Bayes Classifier

>*"a classifier that assigns each observation to the most likely class, given its predictor values"* minimizes the test error rate.

- This lowest error rate is called **Bayes Error Rate**

- Bayes Decision Boundary

- Why not always use Bayes Classifier?

## Moving Forward{.center}

**Keep in mind the good old Bayes Rule**

$$P(A|B) = \frac{P(B|A)* P(A)}{P(B)}$$

## Generative Models - What ?

- We saw that logistic model estimates $Pr(Y=k|X=x)$. 
- Alternatively, we model distribution of each predictor for a given class of Y.
- Then we use the Bayes rule to get $Pr(Y=k|X=x)$
- *"When the distribution of X
within each class is assumed to be normal, it turns out that the model is
very similar in form to logistic regression"*

## Generative Models - Why?

 - For logistic regression, unstable parameter estimates when separation between two classes is substantial.
 - When distribution of X for each class of Y is normal and the sample size is small, these methods do better than logistic regression.
 
## Generative Models - How?

$\pi_k$ is the overall probability of seeing $k^{th}$ class of response in data.

$f_k(X) = Pr(X|Y=k)$

$$Pr(Y=k|X=x) = \frac{\pi_k*f_k(x)}{\sum_{l=1}^K\pi_lf_l(x)}$$


> We are trying to approximate the Bayes classifier!! We will esplore linear discriminant analysis, quadratic discriminant analysis and naive Bayes

## LDA for One predictor

> Over arching goal is to figure out the $f_k(x)$

**To achieve our goal, we assume that $f_k(x)$ is normal.**

$$f_k(x) = \frac{1}{\sigma_k\sqrt{2\pi}}exp(-\frac{1}{2\sigma_k^2}(x-\mu_k)^2)$$

Here, $\mu_k$ and $\sigma_k^2$ is the mean and variance parameter of the $k^th$ class.

we also assume, that $\sigma_1^2 = ...\sigma_K^2$

## LDA for One Predictor

$$
Pr(Y=k|X=x) = \frac{\pi_k*\frac{1}{\sigma\sqrt{2\pi}}exp(-\frac{1}{2\sigma^2}(x-\mu_k)^2)}{\sum_{l=1}^K\pi_l\frac{1}{\sigma\sqrt{2\pi}}exp(-\frac{1}{2\sigma^2}(x-\mu_k)^2)}
$$

$$
log(Pr(Y=k|X=x)) = x.\frac{\mu_k}{\sigma^2}-\frac{\mu_k^2}{2\sigma^2} + log(\pi_k)
$$

$$
x = \frac{\mu_1^2-\mu_2^2}{2(\mu_1-\mu_2)}= \frac{\mu_1 + \mu_2}{2}
$$

## Applying LDA

```{r}
#| echo: true

lda_default_balance_student <-
  MASS::lda(default ~ balance + student, data = Default)
lda_default_balance_student
```

## Applying LDA - training error rate

```{r}
#| echo: true

mean(
  predict(lda_default_balance_student,
          newdata = Default)$class != Default$default
)
```

- training error rate

- trivial null classifier

```{r}
#| echo: true
predict(lda_default_balance_student,
          newdata = Default)|>names()
```

## Exercise

- See the OJ data set in ISLR2

- Use this data set to predict variable purchase

- Split data into 80/20 training and testing.

- Use training data to develop a LDA model. Use RoC and confusion matrix to gauge model effectiveness. Fine tune model. See chapter 9 TMWR.

- predict test data with the fine tuned model.

## QDA{.scrollable}

**Quadratic Discriminant Analysis**

 -  This too assumes that observations within each class are drawn from a Gaussian distribution.
 - However, the assumption of common covariance matrix is not held to be true in QDA. This is where it differs from LDA.
 - This leads to the $x$ in discriminant function to appear as quadratic.
 - Now, $Kp(p+1)/2$  parameters need to be estimated for covariance matrix instead of p(p+1)/2. **This is where bias variance trade off comes to play.**
 - This means LDA can have low variance and high bias, especially if the $\sigma_1^2=....=\sigma_K^2$ assumption is badly off.
 
## Exercise

See the `Smarket` data in ISLR2.

Split in 80/20 training and testing.

Train LDA and QDA models.

Test these models and compare results - use test error rate.

What happens if you take n number of training data sets and n number of testing data sets, run LDA and QDA on each pair and plot training error rate and testing error rate distributions? 
 
## Naive Bayes

  - From LDA and QDA we have seen that estimating $\pi_1...\pi_K$ is easy.
  - Estimating $f_1(x).....f_K(x)$ is difficult.
  - The estimates of LDA and QDA help us avoid estimating a K p-dimensional density functions.
  - The Naive Bayes Classifier makes only one assumption - **Within the kth class, the p predictors are independent.**
  
  $$f_k(x) = f_{k1}(x_1)*f_{k2}(x_2)*...*f_{kp}(x_p)$$


## Naive Bayes

$$pr(X) = \frac{\pi_k*f_{k1}(x_1)*f_{k2}(x_2)*...*f_{kp}(x_p)}{\sum_{l=1}^K \pi_l*f_{l1}(x_1)*f_{l2}(x_2)*...*f_{lp}(x_p)}$$
> How is $f_{kj} estimated?$

## Exercise

use `naiveBayes` function from e1071 package.

Use Smarket data and compared results with QDA.