---
title: "Classificaiton"
subtitle: "Advance Analytics with R (UG 21-24)"
author: "Ayush Patel"
format: 
  revealjs:
    embed-resources: true
    slide-number: c/t
    width: 1400
    theme: [serif, theme.scss]
---
```{r echo=FALSE,message=FALSE,warning=FALSE}

library(tidyverse)
library(MASS)
library(ISLR)
library(ISLR2)
library(patchwork)

```


## Before we start

::: columns
::: {.column width="70%"}

Please load the following packages

```{r}
#| eval: false
#| echo: true

library(tidyverse)
library(MASS)
library(ISLR)
library(ISLR2)

```

<br> <br>

Access lecture slide from [bit.ly/aar-ug](http://bit.ly/aar-ug)
:::

::: {.column width="30%"}
```{=html}
<figure>
<img src="https://images.metmuseum.org/CRDImages/aa/original/DT5334.jpg" alt="Warrior's armor(gusoku)" >
<figcaption style="font-size:10px;">Source: <a herf = "https://www.metmuseum.org/art/collection/search/24975?searchField=All&amp;sortBy=Relevance&amp;high=on&amp;ao=on&amp;showOnly=openAccess&amp;ft=start&amp;offset=0&amp;rpp=40&amp;pos=7">Armor (Gusoku)</a></figcaption>
</figure>
```
:::
:::

## Hello

::: columns
::: {.column width="70%"}
I am [Ayush]{.fragment fragment-index="1" style="font-size:45px"}.

[I am a researcher working at the intersection of data, law, development and economics.]{.fragment fragment-index="2" style="font-size:25px"}

[I teach Data Science using R at Gokhale Institute of Politics and Economics]{.fragment fragment-index="3" style="font-size:25px"}

[I am a [RStudio (Posit) certified tidyverse Instructor.](https://education.rstudio.com/trainers/people/patel+ayush/)]{.fragment fragment-index="4" style="font-size:25px"}

[I am a Researcher at [Oxford Poverty and Human development Initiative (OPHI)](https://ophi.org.uk/), at the University of Oxford.]{.fragment fragment-index="5" style="font-size:25px"}
:::

::: {.column width="30%"}
**Reach me**

{{< fa solid envelope >}} [ayush.ap58\@gmail.com]{style="font-size:25px"}

{{< fa solid envelope >}} [ayush.patel\@gipe.ac.in]{style="font-size:25px"}
:::
:::

## Learning Objective {.center}

*Dip* our toes into classification techniques. How to apply and assess these methods.

References for this lecture:

-   Chapter 4, ISLR (reference)
-   Chapters 9, Intro to Modern Statistics (Reading for intuitive understanding)
-   Chapter 10.2 Modern Data Science with R

## What is Classification?



  - Predict qualitative response
  - Approaches of predicting qualitative response, a process called *classification.*
  - A method or technique can be referred to as a *classifier*.
  - We will look into: *logistic regression, linear discriminant analysis, quadratic discriminant analysis, naive Bayes and K-nearest neighbours*


## What actually happens{.smaller}

>"*....often the methods used for classification first predict the probability that the observation belongs to each of the categories of a qualitative  variable, as the basis for making the classification. In this sense they also behave like regression methods.*"

### Why not use linear regression??

   - Nominal categorical variables have no rank. How to provide quantitative values?
   - Distance between Ordinal variable values are not easy to assign.
   - Could do something when the response is nominal with only two levels.
   - No guarantee that our estimates will be between [0,1]. Makes interpreting probabilities difficult.
   
## `Default` data{.scrollable}

```{r}
#| echo: false

head(Default,20)|>
  knitr::kable()|>
  kableExtra::kable_styling()|>
  kableExtra::scroll_box(width = "100%")
  

```

## Logistic Regression

  - Logistic regressions are well suited for qualitative binary responses.
  - *default* variable from `Default` is our response($Y$).
  - It has two levels `Yes` or `No`.
  - We model the probability that $Y$ belongs to one a particular category.
  - $Pr(default = Yes|balance)$ - logistic model estimates this. Is referred to as $p(balance)$ as well.
  - Mainly, depending on risk aversion behaviour, $a$ is chosen. $p(balance) > a$, where $0<=a<=1$.
  
## But what if ? {.scrollable}

**I ran this:** $p(balance) = \beta_0 + \beta_1X$

:::{.panel-tabset}

### Code

```{r}
#| echo: true

## make a dummy for default

Default|>
  mutate(
    default_dumm = ifelse(
      default == "Yes",
      1,0
    )
  )-> def_dum

## regress dummy over balance and plot 

lm(default_dumm ~ balance, 
   data = def_dum)|>
  broom::augment()|>
  ggplot(aes(balance,default_dumm))+
  geom_point(alpha= 0.6)+
  geom_line(aes(balance, .fitted),
            colour = "red")+
  labs(
    title = "Linear regression fit to qualitative response",
    subtitle = "Yes =1, No = 0",
    y = "prob default status"
  )+
  theme_minimal() -> plot_linear

## Run the logistic regression

glm(
  default_dumm ~ balance,
  data = def_dum,
  family = binomial
)|>
  broom::augment(type.predict = "response")|>
  ggplot(aes(balance,default_dumm))+
  geom_point(alpha= 0.6)+
  geom_line(aes(balance, .fitted),
            colour = "red")+
  labs(
    title = "Logistic regression fit to qualitative response",
    subtitle = "Yes =1, No = 0",
    y = "prob default status"
  )+
  theme_minimal() -> logistic_plot

```


### Output

```{r}
#| echo: false
#| fig-align: center

plot_linear + logistic_plot
```


:::

## Logistic Model{.scrollable}

We saw that some fitted values in the **linear model** were **negative**.

We need a function that will return values between `[0,1]`.

$$p(X) = \frac{e^{(\beta_0 + \beta_1X)}}{1+e^{\beta_0 + \beta_1X}}$$

This is the *logistic function*, modeled by the *maximum likelihood* method.

**odds**:

$$\frac{p(X)}{1-p(X)}$$
**log odds or logit:

$$log(\frac{p(X)}{1-p(X)}) = \beta_0 + \beta_1X$$
 
 
## Exercise - concept

if the following are the results of the model $logit(p(default)) = \beta_0 + \beta_1Balance$:

```{r}
#| echo: false

glm(
  default_dumm ~ balance,
  data = def_dum,
  family = binomial,
)|>
  broom::tidy()|>
  gt::gt()
```
**What is the probability of default with balance $5000??**

## Multiple logistic Regression

$$p(X) = \frac{e^{(\beta_0 + \beta_1X_1 + \beta_2X_2+...+\beta_nX_n)}}{1+e^{\beta_0 + \beta_1X_1 + \beta_2X_2+...+\beta_nX_n}}$$

```{r}


glm(
  default_dumm ~ income + balance + student,
  data = def_dum,
  family = binomial
)|>
  broom::tidy()|>
  gt::gt()|>
  gt::tab_options(
    table.font.size = 20,
    table.width = gt::pct(50)
  )

glm(
  default_dumm ~ student,
  data = def_dum,
  family = binomial
)|>
  broom::tidy()|>
  gt::gt()|>
  gt::tab_options(
    table.font.size = 20,
    table.width = gt::pct(50)
  )

```

## How to know if its good?

> There is no consesus in statistics community over a single measure that can describe a goodness of fit for logistic regression.

```{r}
#| echo: true

glm(
  default_dumm ~ income + balance + student,
  data = def_dum,
  family = binomial
) -> mod_logit

DescTools::PseudoR2(mod_logit,
                    which = c("McFadden", "CoxSnell",
                              "Nagelkerke", "Tjur"))


```

```{r}
#| echo: true

AIC(mod_logit) # be careful with this

```

