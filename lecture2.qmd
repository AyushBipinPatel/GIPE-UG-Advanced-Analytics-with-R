---
title: "Linear Regression"
subtitle: "Advance Analytics with R (UG 21-24)"
author: "Ayush Patel"
format: 
  revealjs:
    embed-resources: true
    slide-number: c/t
    width: 1400
    theme: [serif, theme.scss]
---

```{r echo=FALSE,message=FALSE,warning=FALSE}

library(tidyverse)
library(MASS)
library(ISLR)
library(ISLR2)
library(patchwork)

```


## Before we start

::: columns
::: {.column width="70%"}
\_\_Please load the following packages

```{r}
#| eval: false
#| echo: true

library(tidyverse)
library(MASS)
library(ISLR)
library(ISLR2)

```

<br> <br>

Access lecture slide from [bit.ly/aar-ug](http://bit.ly/aar-ug)
:::

::: {.column width="30%"}
```{=html}
<figure>
<img src="https://images.metmuseum.org/CRDImages/aa/original/DT5334.jpg" alt="Warrior's armor(gusoku)" >
<figcaption style="font-size:10px;">Source: <a herf = "https://www.metmuseum.org/art/collection/search/24975?searchField=All&amp;sortBy=Relevance&amp;high=on&amp;ao=on&amp;showOnly=openAccess&amp;ft=start&amp;offset=0&amp;rpp=40&amp;pos=7">Armor (Gusoku)</a></figcaption>
</figure>
```
:::
:::

## Hello

::: columns
::: {.column width="70%"}
I am [Ayush]{.fragment fragment-index="1" style="font-size:45px"}.

[I am a researcher working at the intersection of data, law, development and economics.]{.fragment fragment-index="2" style="font-size:25px"}

[I teach Data Science using R at Gokhale Institute of Politics and Economics]{.fragment fragment-index="3" style="font-size:25px"}

[I am a [RStudio (Posit) certified tidyverse Instructor.](https://education.rstudio.com/trainers/people/patel+ayush/)]{.fragment fragment-index="4" style="font-size:25px"}

[I am a Researcher at [Oxford Poverty and Human development Initiative (OPHI)](https://ophi.org.uk/), at the University of Oxford.]{.fragment fragment-index="5" style="font-size:25px"}
:::

::: {.column width="30%"}
**Reach me**

{{< fa solid envelope >}} [ayush.ap58\@gmail.com]{style="font-size:25px"}

{{< fa solid envelope >}} [ayush.patel\@gipe.ac.in]{style="font-size:25px"}
:::
:::

## Learning Objective {.center}

Learn to apply and interpret simple and multiple linear regression models.

References for this lecture:

-   Chapter 3, ISLR (reference)
-   Chapters 7 and 8, Intro to Modern Statistics (Reading for intuitive understanding)

## Advertising Data

```{r}
#|echo: false

readr::read_csv("data/Advertising.csv") -> advertisement
```

```{r show-advr}

advertisement|>
  knitr::kable()|>
  kableExtra::kable_styling()|>
  kableExtra::scroll_box(width = "100%", 
                         height = "600px")
  

```

## Association between slaes and budget?

```{r}
#|echo: false

advertisement|>
  ggplot2::ggplot()+
  ggplot2::geom_point(
    ggplot2::aes(
    x = TV, y = sales
  ), colour = "red",
  alpha = 0.5)+
  ggplot2::geom_point(
    ggplot2::aes(
    x = radio, y = sales
  ), colour = "green",
  alpha = 0.5)+
  ggplot2::geom_point(
    ggplot2::aes(
    x = newspaper, y = sales
  ), colour = "steelblue",
  alpha = 0.5)+
  ggplot2::labs(
    title = "Is there Association between Sales and advertisement budget??",
    subtitle = "red is TV, green is radio and blue is newspaper",
    x = "advertisement budget"
  )+
  ggplot2::theme_bw()
```

## How strong is the association, if any? {.center}

**sales and TV**

```{r}
#|echo: false
cor(x = advertisement$TV,
    y = advertisement$sales)
```

**sales and radio**

```{r}
#|echo: false
cor(x = advertisement$radio,
    y = advertisement$sales)
```

**sales and newspaper**

```{r}
#|echo: false
cor(x = advertisement$newspaper,
    y = advertisement$sales)
```

## Linear model {.center}

A linear model can help us answer questions about association between response and predictors, predict sales in future, linearity of relation, and interaction between predictors.

## A simple linear model

$$
Y \approx \beta_0 + \beta_1X
$$

$$\beta_0\hspace{1mm} is\hspace{1mm}population\hspace{1mm}intercept$$

$$\beta_1\hspace{1mm} is\hspace{1mm}population\hspace{1mm}slope$$ Our estimates are represented as :

$$ \hat\beta_0$$ $$\hat\beta_1$$

## How to reach the best estimate?

**The Idea is to, essentially, draw a line through the points such that distance of every point from line is as small a possible.**

```{r}

advertisement|>
  ggplot2::ggplot(ggplot2::aes(
    x = TV, y = sales
  ))+
  ggplot2::geom_point(
    colour = "red",
  alpha = 0.5)+
  ggplot2::geom_smooth(method = "lm",
                       se =F,
                       size = 0.51)+
  ggplot2::labs(
    title = "What is the best fit?",
    x = "TV budget"
  )+
  ggplot2::theme_bw()
```

## Least squares

One way to get estimates of population coefficients or parameters is **minimizing least squares**.

$$sales \approx \beta_0 + \beta_1*TV$$

$$\hat y_i = \hat\beta_0 + \hat\beta_1x_i$$ $$e_i = y_i - \hat y_i$$

$$RSS = e_1^2 + e_2^2....+e_n^2$$

## Minimize RSS

*Least square coefficient estimates*

$$
\hat\beta_1 = \frac{\sum_i^n(x_i - \bar x)(y_i - \bar y)}{\sum_i^n(x_i - \bar x)^2}
$$

$$
\hat\beta_0 = \bar y - \hat\beta_1\bar x 
$$

## The model

```{r}
lm(sales ~ TV, data = advertisement)
```

"For every\`additional \$1000 spent on TV advertisement budget, there is additional sale of \~47.5 units"

## Exercise

Use the data `Auto` from the `{ISRL2}` Fit this model.

$$horsepower = \beta_0 +  \beta_1*weight + \epsilon$$

find coeff estimates and residuals: $$\hat\beta_0$$ and $$\hat\beta_1$$

## How well did we estimate the coefficients? {.scrollable}

$$Compute\hspace{1mm} standard\hspace{1mm}error\hspace{1mm} of\hspace{1mm} \hat\beta_0\hspace{1mm} and\hspace{1mm} \hat\beta_1$$

something like this:

$$Var(\hat\mu) = SE(\hat\mu) = \frac{\sigma^2}{n}$$

**but in reality**

$$SE(\hat\beta_0)^2 = \sigma^2[\frac{1}{n}+\frac{\bar x^2}{\sum_i^n(x_i - \bar x)^2}]\hspace{2cm}SE(\hat\beta_1)^2 = \frac{\sigma^2}{\sum_i^n(x_i - \bar x)^2}$$

What is sigma here ?

$$what\hspace{1mm} happens\hspace{1mm} when\hspace{1mm} x_i\hspace{1mm} are\hspace{1mm} spread\hspace{1mm} out\hspace{1mm} ?$$

**We can use SE to to hypothesis testing. t-statistic is used to do this in practise**

$$t = \frac{\hat\beta_1 - 0}{SE(\hat\beta_1)}$$

```{r}
summary(lm(sales ~ TV, data = advertisement))
```

## Assessing Model Accuracy{.scrollable}

**RSE** and **R\^2**

```{r echo=TRUE}
mod <- summary(lm(sales ~ TV, data = advertisement))
```

```{r echo=TRUE}

mod$sigma

mod$sigma/mean(advertisement$sales)

mod$r.squared

```

:::{.panel-tabset}

### RSE

Residual Standard Error is the standard deviation of $\epsilon$. Essentially $\sqrt\frac{RSS}{n-2}$

This is a measure of *lack of fit*.

It is in terms of Y, so scale is involved(Beware).

### $R^2$

It is the proportion of variance in the response that is explained by the model.

TSS is total sum of squares $\sum_i^n(y_i - \bar y)$

$$\frac{TSS-RSS}{TSS}$$

$R^2$ is between 0 and 1. Therefore easier to interpret.

:::

## Multiple Linear Regression{.center}

   - In reality, there can be more than one predictors that influence or are associated with response.
   - Run a simple linear regression for each predictor? Why not? (Cant make a single prediction about sale, in every model two other medias are ignored - predictors could be correlated )
   
## Multiple Linear Regression{.scrollable}

```{r echo=TRUE}

mod <- lm(sales ~ TV + radio + newspaper,
   data = advertisement)

broom::tidy(mod)

broom::glance(mod)

broom::augment(mod)

```

## What is going on with newspaper?{.scrollable}

  - We saw that sales and newspaper have a positive weak correlation (~0.22).
  - Then Why is the $\beta_(newspaper)$ negative?
  - What do you think about its standard error?
  - What is the coefficient and SE when you execute `lm(sales ~ newspaper, data = advertisement)`? Why is this so different?
  
```{r echo=TRUE}
lm(sales~newspaper, data = advertisement)|>
  summary()|>
  broom::tidy()
```
  
## Fishy business

```{r}
corrplot::corrplot(corr = cor(advertisement[,-1]),
                   title = "Advertisement Correlation Matrix",
                   method = "number")
```
## The F-statistic

$$
F = \frac{(TSS-RSS)/p}{RSS/(n-p-1)}
$$
In a multiple linear regression we need to ensure that all coefficients for $p$ predictors are not equal to zero. Meaning that at least one of those is non-zero. F-statistic helps us ensure that.

That means if $H_0$ is true, F will be near 1 and if not F will be far larger than 1.

## Variable Selection

Well if one or more predictors have non-zero coef and are associated with response, **Which are those?**

   - We will begin with trying out various subsets of predictors and assess all possible models.(Can you try out $2^p$ models?)
   - Mallow's $C_p$, Akaike information criterion (AIC), Bayesian information criterion (BIC), and adjusted $R^2$.
  - Forward selection, Backward Selection
  
## Qualitative Variables - dummies

```{r}
ISLR2::Credit|>
  knitr::kable()|>
  kableExtra::kable_styling()|>
  kableExtra::scroll_box(width = "100%", height = "600px")
  
```

## Exercise 

Use `Credit` in {ISLR2} data, use balance as response and create a model that you think is good. Try out qualitative variables as well. Begin by looking at all possible variables in data and think which could affect the response.

TRY the {broom} package as well.

## Extending the linear model

  - **Additive property** ($X_j$ may be associated with response in a manner that changes with different values of other predictors )
  - **Linear property** (Response will experience the same amount of changes with one unit change in a predictor, regardless of the value of predictor.)
  
> We will see some "classic" ways of relaxing these properties.

## Breaking Additive Assumption by Interaction

Instead of $Y = \beta_0 + \beta_1X_1 + \beta_2X_2$

We add a interaction term for $X_1$ and $X_2$

$$
\begin{align}
\begin{aligned}
Y & = \beta_0 + \beta_1X_1 + \beta_2X_2 + \beta_3X_1X_2\\
  & = \beta_0 + X_1(\beta_1 + \beta_3X_2)+\beta_2X_2
\end{aligned}
\end{align}
$$

*What to do when p-values of $\beta_1$ and $\beta_2$ are large and the p-value of the interaction term is small? (hierarchical principle)*

## Advertisement and Interaction{.scrollable}

::: panel-tabset

### Sales, TV and radio

```{r echo=FALSE,warning=FALSE}
advertisement|>
  ggplot2::ggplot(ggplot2::aes(TV,sales))+
  ggplot2::geom_point(ggplot2::aes(colour = radio))+
  ggplot2::geom_smooth(method = "lm")+
  ggplot2::scale_color_viridis_c(option = "A")+
  ggplot2::guides(colour = ggplot2::guide_colorbar(
    title = "Radio",
    direction = "horizontal",
    barwidth = 15))+
  ggplot2::theme_minimal()+
  ggplot2::theme(
    legend.position = "top"
  )
```


### The old model

```{r echo=TRUE}
old_nmod <- lm(sales~TV+radio, data=advertisement)

broom::tidy(old_nmod)|>
  gt::gt()

broom::glance(old_nmod)
```


### Interaction

```{r echo=TRUE}
new_nmod <- lm(sales~TV+radio+TV*radio,
               data=advertisement)

broom::tidy(new_nmod)|>
  gt::gt()

broom::glance(new_nmod)
```


:::

## Exercise{.smaller}

Use the `Credit` data in the {ISRL2} library.

> How does interation work when the variables of interest are QL * QN or QL * QL

**Create a dummy for `student`**

compare these two models

$$Y = \beta_0 + \beta_1income + \beta_2dummy\_student +  \beta_3(income*dummy\_student)$$ 
and 
$$Y = \beta_0 + \beta_1income + \beta_2dummy\_student$$

## Polynomial Regression

:::panel-tabset

### The chart

```{r echo=FALSE,warning=FALSE}

ISLR2::Auto|>
  ggplot2::ggplot(ggplot2::aes(horsepower,mpg))+
  ggplot2::geom_point()+
  ggplot2::theme_minimal()+
  ggplot2::labs(
    title = "Does this look linear?"
  )

```
### quadratic 

$mpg ~ \beta_0 + \beta_1horsepower + \beta_2horsepower^2$

```{r}

ISLR::Auto|>
  dplyr::mutate(
    horsepower_2 = horsepower^2
  ) -> A2

quad_mod <- lm(mpg~ horsepower + horsepower_2,
               data = A2)

broom::tidy(quad_mod)|>
  gt::gt()

broom::glance(quad_mod)

```

### simple 

$mpg ~ \beta_0 + \beta_1horsepower$

```{r}
simp_mod <- lm(mpg~horsepower,
               data = ISLR::Auto)

broom::tidy(simp_mod)|>
  gt::gt()

broom::glance(simp_mod)

```
:::

## Potential Problems

::::{.columns}

:::{.column width="60%"}
  - Non-linearity of the response-predictor relationships.
  - Correlation of error terms.
  - Non-constant variance of error terms.
  - Outliers.
  - High-leverage points.
  - Collinearity.
:::

:::{.column width="40%"}
<iframe src="https://giphy.com/embed/mTXSKTk3BsT6w" width="480" height="360" frameBorder="0" class="giphy-embed" allowFullScreen></iframe><p><a href="https://giphy.com/gifs/mTXSKTk3BsT6w">via GIPHY</a></p>
:::

::::


## Potential problems - Non linear Data

**Residual vs Fitted**, if there is indication of non-linear relation, try non-linear transformations on you predictors.

```{r echo=FALSE, warning=FALSE,fig.align='center'}

broom::augment(quad_mod)|>
  ggplot(aes(.fitted, .resid))+
  geom_point()+
  geom_smooth()+
  theme_minimal()+
  labs(title = "For quadratic linear regression") -> p1

broom::augment(simp_mod)|>
  ggplot(aes(.fitted, .resid))+
  geom_point()+
  geom_smooth()+
  theme_minimal()+
  labs(title = "For simple linear regression") -> p2

p1+p2

```

## Potential Problems - Correlated Error Terms

  - We assume that $\epsilon_1, \epsilon_2, ...\epsilon_n$ are uncorrelated. This means that on reasonable deduction can be done about $\epsilon_{n+1}$ from the information we have about $\epsilon_n$.
  - What if they are?
  - recall that $SE$ is calculated based on this assumption.
  - This means that in case there is correlation in error terms, we may end up trusting the model more than we should.
  - This is often seem in time-series 
  - How to test - Durbin-Watson test, Ljung-Box Q test
  
## Potential Problems - Non-Constant Variance of error term

  - This means that $Var(\epsilon_i)$ could increase or decrease with the response.
  - If this is found, this could overestimate $SE$.
  - Can be identified with a funnel shape of the residual vs fit chart.
  - To address this, use of a concave function, e.g. log(Y) or $\sqrt{Y}$. 
  
## Potential Problem - Outlier

> "*An outlier is a point for which $Y_i$ is far from the value predicted by the model.*"

  - If the predictor value for an Outlier is usual, then it has little effect on the fit, but can effect RSE, p-values and $R^2$.
  - We can use residual vs fitted chart to identify an Outlier. But the question remains: *How high is high?*
  - To answer that, plot studentized residuals. How are these calculated? `studres(model)`. For each residual this value is expected to be less that absolute 3, i.e. between -3 and 3.
  
## Potential Problems - High leverage

> When the predictor is unusually high, those point are called observations with high leverage.

 - High leverage points have high impact on the fit line.
 - A point can be high leverage even if it is usual for each individual predictor but not as a set predictors.
 - `leverage` is calculated to quantify this (for one predictor).
 
 $$h_i = \frac{1}{n}+\frac{(x_i - \bar{x})^2}{\sum_{i'=1}^{n}(x_{i'} - \bar{x})}$$
 
 - $h_i$ is between $\frac{1}{n}$ and $1$
 - "the average leverage for all the observations is
always equal to $\frac{(p+1)}{n}$."

## Potential Problems - Collinearity

